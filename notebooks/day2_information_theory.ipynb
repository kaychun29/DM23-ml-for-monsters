{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5ba4acf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning Machines: Information Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d33a14a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Code Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abf289c1-7858-4edf-b655-66678ec068ab",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc7a5050-012d-4e55-b6c2-633f3b72a39f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cedd1dbf-5a49-4a05-a05d-9b79477c1ec7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../py')\n",
    "\n",
    "from plots import (\n",
    "    plot_zipf_dist, \n",
    "    compare_distributions_in_plot,\n",
    "    plot_bernoulli,\n",
    "    plot_discrete_distribution,\n",
    "    plot_entropy\n",
    ")\n",
    "\n",
    "sns.set_style(\"ticks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "342bd329",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33dc568-fb04-4a22-a92b-ba375da3bb14",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Resources\n",
    "\n",
    "### Textbooks\n",
    "\n",
    "- JR Pierce (1980). [*An Introduction to Information Theory – Symbols, Signals and Noise*](http://libgen.rs/book/index.php?md5=B7FA722640006DC4F64D4447FBC051E9). Second Revised Ed. Dover.\n",
    "- TM Cover & JA Thomas  (2006) [*Elements of Information Theory*](http://libgen.rs/book/index.php?md5=F84D706DD712F25317DF40949026F072). Hoboken, N. J.: John Wiley & Sons. \n",
    "- KP Murphy (2021) [*Probabilistic Machine Learning: An Introduction*](https://probml.github.io/pml-book/book1.html). MIT Press"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb8a5a3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Historical\n",
    "\n",
    "- S Kullback S & RA Leibler (1951) [On Information and Sufficiency](https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-22/issue-1/On-Information-and-Sufficiency/10.1214/aoms/1177729694.full). *The Annals of Mathematical Statistics* 22(1):\n",
    "79–86.\n",
    "- Claude Shannon (1945) [*A Mathematical Theory of Cryptography - Case 10878*](https://www.iacr.org/museum/shannon/shannon45.pdf).\n",
    "- Claude Shannon (1948a) [A Mathematical Theory of Communication](https://ieeexplore.ieee.org/document/6773024). The Bell System Technical Journal 27(3): 379–423.\n",
    "- Claude Shannon (1948b) [A Mathematical Theory of Communication](https://onlinelibrary.wiley.com/doi/10.1002/j.1538-7305.1948.tb01338.x). The Bell System Technical Journal 27(4): 623–656.\n",
    "- Claude Shannon & Warren Weaver (1963): [*The Mathematical Theory of Communication*](https://monoskop.org/images/b/be/Shannon_Claude_E_Weaver_Warren_The_Mathematical_Theory_of_Communication_1963.pdf). Illinois University Press."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04a1b9f-21d9-4f35-96a1-56246830b931",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Premise: Language As Distribution\n",
    "\n",
    "> A language is considered for cryptographic purposes to be a stochastic process which produces a discrete sequence of symbols in accordance with some systems of probabilities. Associated with a language there is a certain parameter $D$ which we call the redundancy of the language. $D$ measures, in a sense, how much a text in the language can be reduced in length without losing any information. As a simple example, if each word in a text is repeated a reduction of 50 per cent is immediately possible. Further reductions may be possible due to the statistical structure of the language, the high frequencies of certain letters or words, etc. The redundancy is of considerable importance in the study of secrecy systems. - Claude Shannon (1945, p. 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc98613-f35f-4e1a-bafc-4e28158fa51d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's Unpack That!\n",
    "\n",
    "- Language consists of discrete symbols\n",
    "- These symbols are generated by a system of probabilities\n",
    "- This system is not entirely random or \"uniform\", it has *structure* or ***redundancy***\n",
    "- Consequently: \"The redundancy is of considerable importance in the study of secrecy systems\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaefb88-7897-44d4-9ab2-180acf80ba00",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### What We'll Learn Today\n",
    "\n",
    "- We'll revisit the idea of language as distribution\n",
    "- We'll get a broad overview of probability distributions in general\n",
    "    - Uniform, Bernoulli, Binomial, Normal\n",
    "- Bits for counts\n",
    "    - Aside: What is the logarithm?\n",
    "- Bits for probabilities\n",
    "- Information entropy\n",
    "- Relative entropy and redundancy\n",
    "- Kullback-Leibler Divergence\n",
    "- Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad7da21-48c0-4650-9159-aaa9ae0fb487",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why?\n",
    "\n",
    "- There would be ***no* machine learning *without* information theory**\n",
    "- Most models you will ever use hinge on **cross-entropy** loss\n",
    "    - A way to measure the difference between *observed* and *expected* behaviour\n",
    "    - Used to optimize a very large number of machine learning models\n",
    "- These concepts are often considered hard\n",
    "    - **My take:** Because they are taught poorly and hastily!\n",
    "    - If we start from fundamentals, we can build an intuitive understanding of them\n",
    "- Understand how machine learning models perceive the world!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3218636-ac29-4263-92ff-ef9d78bf4357",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Zipf's Law\n",
    "\n",
    "- Already in 1932, the linguist George Zipf had observed that word frequency is inversely proportional to work rank\n",
    "- In other words: There's a statistical regularity to how frequent words are in text\n",
    "- Let's take an example using Lewis Carroll's \"Alice in Wonderland\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecbc3762-ca35-4c43-8ddd-60c91444104c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ff096cd-f16f-44f7-95f7-de48e20cd511",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def tokens_from_gutenberg_text(txt_name):\n",
    "    sents = gutenberg.sents(txt_name)\n",
    "    words = [word for sent in sents for word in sent]\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(\" \".join(words).lower())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fd48234-2300-4c9c-851f-5aaddbfab7f9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/home/jovyan/nltk_data'\n    - '/opt/conda/nltk_data'\n    - '/opt/conda/share/nltk_data'\n    - '/opt/conda/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m alice_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtokens_from_gutenberg_text\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcarroll-alice.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m alice_tokens[:\u001b[38;5;241m10\u001b[39m]\n",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m, in \u001b[0;36mtokens_from_gutenberg_text\u001b[0;34m(txt_name)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokens_from_gutenberg_text\u001b[39m(txt_name):\n\u001b[1;32m      2\u001b[0m     sents \u001b[38;5;241m=\u001b[39m gutenberg\u001b[38;5;241m.\u001b[39msents(txt_name)\n\u001b[0;32m----> 3\u001b[0m     words \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mword\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msents\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msent\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      4\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m RegexpTokenizer(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(words)\u001b[38;5;241m.\u001b[39mlower())\n",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokens_from_gutenberg_text\u001b[39m(txt_name):\n\u001b[1;32m      2\u001b[0m     sents \u001b[38;5;241m=\u001b[39m gutenberg\u001b[38;5;241m.\u001b[39msents(txt_name)\n\u001b[0;32m----> 3\u001b[0m     words \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sents \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m sent]\n\u001b[1;32m      4\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m RegexpTokenizer(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(words)\u001b[38;5;241m.\u001b[39mlower())\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/nltk/corpus/reader/util.py:306\u001b[0m, in \u001b[0;36mStreamBackedCorpusView.iterate_from\u001b[0;34m(self, start_tok)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_toknum \u001b[38;5;241m=\u001b[39m toknum\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_blocknum \u001b[38;5;241m=\u001b[39m block_index\n\u001b[0;32m--> 306\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_block\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tokens, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m, AbstractLazySequence)), (\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock reader \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m() should return list or tuple.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_block\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m    310\u001b[0m )\n\u001b[1;32m    311\u001b[0m num_toks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokens)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/nltk/corpus/reader/plaintext.py:126\u001b[0m, in \u001b[0;36mPlaintextCorpusReader._read_sent_block\u001b[0;34m(self, stream)\u001b[0m\n\u001b[1;32m    121\u001b[0m sents \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m para \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_para_block_reader(stream):\n\u001b[1;32m    123\u001b[0m     sents\u001b[38;5;241m.\u001b[39mextend(\n\u001b[1;32m    124\u001b[0m         [\n\u001b[1;32m    125\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m--> 126\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sent_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m(para)\n\u001b[1;32m    127\u001b[0m         ]\n\u001b[1;32m    128\u001b[0m     )\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sents\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/nltk/data.py:903\u001b[0m, in \u001b[0;36mLazyLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr):\n\u001b[0;32m--> 903\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    904\u001b[0m     \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    905\u001b[0m     \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/nltk/data.py:895\u001b[0m, in \u001b[0;36mLazyLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__load\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 895\u001b[0m     resource \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;66;03m# This is where the magic happens!  Transform ourselves into\u001b[39;00m\n\u001b[1;32m    897\u001b[0m     \u001b[38;5;66;03m# the object by modifying our own __dict__ and __class__ to\u001b[39;00m\n\u001b[1;32m    898\u001b[0m     \u001b[38;5;66;03m# match that of `resource`.\u001b[39;00m\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m \u001b[38;5;241m=\u001b[39m resource\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/home/jovyan/nltk_data'\n    - '/opt/conda/nltk_data'\n    - '/opt/conda/share/nltk_data'\n    - '/opt/conda/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "alice_tokens = tokens_from_gutenberg_text('carroll-alice.txt')\n",
    "alice_tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00641d27-c2ae-4c38-96b5-c8cbc4400294",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "alice_counts = Counter(alice_tokens)\n",
    "alice_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b3b25b-5244-40a6-aa09-c031698099bc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def get_probs_from_counter(counts):\n",
    "    sum_ = sum(counts.values())\n",
    "    probs = Counter()\n",
    "    for token in counts:\n",
    "        probs[token] = np.float32(np.format_float_positional(counts[token] / sum_))\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35167a49-eac8-4412-ac75-8898d4ab0f49",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "alice_probs = get_probs_from_counter(alice_counts)\n",
    "alice_probs.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acecec06-ac8a-4548-889e-15807af890f8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def zipf_to_uniform(top_probs, n):\n",
    "    return Counter({key: 0.5 for key in dict(top_probs[:n])})\n",
    "\n",
    "alice_uniform = zipf_to_uniform(alice_probs.most_common(), 10)\n",
    "alice_uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d65422-eac2-4a40-b14f-586a0e0fe8fd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_zipf_dist(alice_probs, 'bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fd71b4-d56a-408d-9310-8963d05d2f6c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We could also plot the language using a line. This will be useful later when we want to compare distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed24d9d2-88e8-42bb-b956-b492ecd0e388",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_zipf_dist(alice_probs, 'line')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfb756d-3a7a-4560-b3f6-bad1b2a868ad",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Uniform Distribution\n",
    "\n",
    "- What if we instead plot the uniform probabilities?\n",
    "- We get what is known as the \"uniform distribution\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8a2fac-2bc2-483e-bd36-494db76e3b8c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_zipf_dist(zipf_to_uniform(alice_probs.most_common(), 50), 'bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091198fc-119c-426c-9d54-57987a1e2560",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Uniform Distribution\n",
    "\n",
    "- Now there is no structure or redundancy\n",
    "- There are words, but could we even distinguish their \"meaning\" if language behaved like this?\n",
    "- Probably not!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb77dc05-2d32-42d9-90cb-6632d96d3b94",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Aside: The Limits of Zipf's Law\n",
    "\n",
    "- However, as we've already seen something like Zif's law won't generate what we would recognize as \"language\"\n",
    "- We will need more complex approximations, which is where neural nets come in the picture\n",
    "- Nonetheless: We can use Zipf's Law to get an understanding of how we want to proceed to compare *distributions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbc21b8-2991-47bb-ae2e-9efd830201e6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def zipf_unigram_lm(probs, n):\n",
    "    output = ''\n",
    "    for i in range(0,n):\n",
    "        predicted_word = np.random.choice(list(probs.keys()), \n",
    "                                          1, \n",
    "                                          p=list(probs.values()))[0]\n",
    "        output += predicted_word + ' '\n",
    "    return output.strip()\n",
    "\n",
    "zipf_unigram_lm(alice_probs, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4fa641-f319-45e3-b9fc-f60cdfbc48a1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## What Are Distributions?\n",
    "\n",
    "- Before we move onward to information, let's take a moment to really understand distributions\n",
    "- Why? Because, as we will see, **information** measures the *properties of distributions*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c00d0d7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Generating Distributions with Numpy\n",
    "\n",
    "- We want to see how some function behavs across the x-axis\n",
    "    - We need to generated a bunch of x-values\n",
    "    - Our x-axis measures the probabilitiy of heads for different distributions\n",
    "    - So it goes from 0 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6fbde4-3a3d-429b-ab47-3dd39b12dcfb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "p_heads_interval_10 = np.linspace(0, 1, 10)\n",
    "p_heads_interval_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab83818-7bdb-48ef-8c46-c55d5d277f1a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(list(range(0, len(p_heads_interval_10))), p_heads_interval_10)\n",
    "plt.xlabel(\"nth simulation\", fontsize=16)\n",
    "plt.ylabel(\"P(H)\", fontsize=16)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aff5ae-7bd5-4b56-8c0b-eccce3b92374",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Now we can actually generate a distribution\n",
    "- For that, let's go back to our old friend the **coin toss**\n",
    "- We let the probability of heads increase in small increments from 0 to 100\n",
    "- That gives us a list of different values for $P(H)$\n",
    "- Now we can get a similar sized but *inverse* list of $P(T)$ by just taking $1 - P(H)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7852a780",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**PROMPT:** Generate ```p_heads``` using ```linspace``` and then use ```p_heads``` to create ```p_tails```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88bc3fb-7e9d-4607-a18c-61405f4533d4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "p_heads = np.linspace(0, 1, 100)\n",
    "p_tails = 1 - p_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cb3178",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(list(range(0, len(p_heads))), p_heads)\n",
    "plt.xlabel(\"nth simulation\", fontsize=16)\n",
    "plt.ylabel(\"P(H)\", fontsize=16)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f106971d-213e-4642-b6fb-9cab72e1b073",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "compare_distributions_in_plot(list(range(0, 100)),\n",
    "                              p_heads, \n",
    "                              p_tails,\n",
    "                              r'$P(H)$',\n",
    "                              r'$P(T)$',\n",
    "                              \"nth Simulation\",\n",
    "                              \"Probability\",\n",
    "                              \"Probabilities of heads and tails\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4692784d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- As expected, $P(H)$ decreases as $P(T)$ increases and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f3d318",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (10,7)\n",
    "plt.scatter(p_tails, p_heads, label = \"P(H) vs P(T)\")\n",
    "plt.xlabel(\"P(T)\", fontsize=16)\n",
    "plt.ylabel(\"P(T)\", fontsize=16)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.legend(fontsize=14)\n",
    "plt.title(\"P(H) as a function of P(T)\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6186f96b-e123-4a80-8ab2-d43a1a1af057",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Distributions = Models\n",
    "\n",
    "- Distributions are also **models**\n",
    "    - Assumptions about how our data is distributed\n",
    "    - Once we have a distribution, we can \"draw\" or \"simulate\" from it, i.e use it to generate *new* data\n",
    "- This is what a language model does when it predicts!\n",
    "- Let's look at some well known distributions to clarify the point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403da44f-346e-47bf-b56d-8ca80f3d9e43",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### The Bernoulli Distribution\n",
    "\n",
    "- What would we do instead of just looking at a single coin, wanted to do an experiment with 100 trials?\n",
    "- What is our **model** here, i.e. what distribution do we assume our data is generated from?\n",
    "    - This type of experiment with two outcomes simulates from the so called **Bernoulli distribution**\n",
    "- Let's take a look!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbc2ab9-5149-4bb9-84b6-c212735f2e7b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- First, we want to define a function to toss our coin. Afterall, we'll be tossing it 100 times!\n",
    "\n",
    "**PROMPT:** ```random.random()``` generates a random value between 0 and 1. Use this to build a function that takes the value ```p```. ```p``` is the bias of the coin, i.e. 0.5 for a fair coin. The function should use ```random.random()``` to return the strings \"heads\" or \"tails\". <div>\n",
    "**Hint:** Use ```if``` and ```else```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4c5ea8-a0b6-4453-ac12-446e3cf373b0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def coin_toss(p):\n",
    "    if random.random() >= p:\n",
    "        coin = \"tails\"\n",
    "    else:\n",
    "        coin = \"heads\"\n",
    "    return coin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c967d7d9-320e-46a0-bfec-8c8d2ed525ce",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "coin_toss(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3e8a55-cdd6-434d-9d8e-e82592244e91",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Now, let's repeat that $n$ times, where $n$ is the number of trials in our experiment (in our case 100)\n",
    "- For each trial, we keep a tally, heads or tails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3407846b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**PROMPT:** Write a function ```count_outcomes```. It should contain a loop that calls ```coin_toss``` to generate ```n``` coin tosses at some ```p_heads```. Give ```n``` and ```p_heads``` as inputs to the function and return a ```Counter```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9780e7c0-dbce-4378-a3fb-5e613f6c1e58",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def count_outcomes(n=100, p_heads=0.5):\n",
    "    outcomes = []\n",
    "    for i in range(0,n):\n",
    "        outcome = coin_toss(p_heads)\n",
    "        outcomes.append(outcome)\n",
    "    return Counter(outcomes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c040f1-6ef5-43f4-8ef0-266587d8eb4b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We set the size of $n$ but also the probability of heads\n",
    "- We will keep using an unbiased coin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14787c9-441a-47f2-8a64-2d1881ed558a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n_trials = 100\n",
    "p = 0.5\n",
    "count_outcomes(n=n_trials, p_heads=p)['heads']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27330ffd-3780-40f1-ba62-3d19b87ff837",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Now we can simulate and plot an outcome from our distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbdb862-9d86-45f5-be4a-3620471010e7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "heads = count_outcomes(n=n_trials, p_heads=0.5)['heads']\n",
    "tails = n_trials - heads\n",
    "plot_bernoulli([heads, tails])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5760d090-5472-4141-9ca5-484f1035c914",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Let's try that with a biased coin, $P(H) = 0.7$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaa1db8-1c2a-4158-b0cc-17e93d7a5770",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "heads = count_outcomes(n=n_trials, p_heads=0.7)['heads']\n",
    "tails = n_trials - heads\n",
    "plot_bernoulli([heads, tails])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c4bc63-2990-42e9-837f-7b8fed05364e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Binomial Distribution\n",
    "\n",
    "- However, as we've seen our simulation is still quite unstable.\n",
    "- What if we now instead repeated that experiment 10 times?\n",
    "- So: 10 experiments, each with 100 coin tosses\n",
    "- This type of experiment will draw from the **binomial distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b18f258-2c2b-4a9e-8a21-507050e89040",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def simulate(ns=100, n=100, p=0.5):\n",
    "    sims = []\n",
    "    for i in range(0,ns):\n",
    "        outcomes = count_outcomes(n, p)\n",
    "        sims.append(outcomes)\n",
    "    return [(s['heads'], s['tails']) for s in sims]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51bbab7-6c22-47c0-ab06-e733050aad31",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now we have to add the number of simulations, in addition to the number of trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1f4a50-37b0-453f-8d4b-d65a064ff0e6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n_sims = 1000\n",
    "n_trials = 10\n",
    "sims = simulate(ns=n_sims, n=n_trials, p=0.2)\n",
    "\n",
    "n_heads = [s[0] for s in sims]\n",
    "n_tails = [s[1] for s in sims]\n",
    "\n",
    "sims_df = pd.DataFrame(data={'heads':n_heads, \n",
    "                             'tails':n_tails})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac697cff-eb61-4547-bcd3-d9972c29221e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We have one row for each trial, and one column for each outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b75a2d-4da1-4646-be77-4343cc1be973",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(sims_df.shape)\n",
    "sims_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b3e1ac-9647-4a78-b96e-63a1435e16ab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When we plot it, we notice it's slightly skewed. This is because our probability is skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137f5bd6-c99a-45d4-a516-7600a42846d1",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_discrete_distribution(sims_df, \n",
    "                           'heads', \n",
    "                           f'The number of heads in {n_trials} coin tosses over {n_sims} simulations.',\n",
    "                           'It looks like a binomial distribution.',\n",
    "                           'N heads',\n",
    "                           'Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223dde05-926d-4e90-a9a8-6c51d6038050",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Normal Distribution & Central Limit Theorem\n",
    "\n",
    "- A key result in statistics is if we have enough repetitions, many distributions start to look like the **normal distribution**\n",
    "- This is known as the **Central Limit Theorem**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37f98dc-e766-4903-a645-e1e9a47c9f57",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "n_sims = 1000\n",
    "n_tosses = 1000\n",
    "\n",
    "sims = simulate(ns=n_sims, n=n_tosses, p=0.2)\n",
    "\n",
    "n_heads = [s[0] for s in sims]\n",
    "n_tails = [s[1] for s in sims]\n",
    "\n",
    "sims_df = pd.DataFrame(data={'heads':n_heads, \n",
    "                             'tails':n_tails})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a644721c-3e4a-477e-bbda-1e4dacc2d080",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_discrete_distribution(sims_df, \n",
    "                           'heads', \n",
    "                           f'The number of heads in {n_tosses} coin tosses over {n_sims} simulations.',\n",
    "                           'Now it looks like a normal distribution!',\n",
    "                           'N heads',\n",
    "                           'Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d76746-f9c4-4082-8467-9da5c6078689",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Representing Distributions with Bits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39518102-ec1f-4537-aa75-22b78086e5a5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Okay, now we've established some key things:\n",
    "    - We know what a distribution is\n",
    "    - We know that Shannon and other pioneers of cybernetics thought of language as a stochastic process \n",
    "        - Ergo: Language was a distribution for them \n",
    "    - If we know some of the statistical regularities or the \"redundancies\" of this distribution we can work on problems like cryptography and machine translation.\n",
    "- However: In order to do anything practical with this information, our distributions of interested have to be somehow **represented**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131d7ba6-4ae6-41eb-9735-732956357c84",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Let's revisit the image of the noisy channel. When an information source decides to send a signal, it has to be somehow encoded\n",
    "- Basing his ideas on innovations in \"Boolean algebra\", Shannon decided that the most intuitive way to represent anything was the most simple one: as \"yes\" or \"no\", 0 or 1.\n",
    "- A poll in the Bell Labs coffee room established that this unit should be known as a \"bit\" of information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4ff4be-4bdd-4c1f-8b8b-f8d4fe53d1ec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<div>\n",
    "<img src=\"../img/shannon_general_communication_system_diagram.png\" width=\"80%\"\n",
    "     caption=\"test\"/><p>\n",
    "    <figcaption>Shannon's diagram for a \"general communication system\".</figcaption>\n",
    "</div>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f544306-79fc-4219-86bd-ed0875cb4e9a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Let's follow the structure in Shannon's seminal papers and work up our intuition on bits from the most simple possible example to more complex ones\n",
    "    - The coin toss returns!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e910bd74-43c0-41d1-85bd-41763a48f58a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**PROMPT:** If we want to represent a coin toss as bits, how do we proceed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ff02cf-ea5e-47da-aa4f-6e48f63b7030",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def print_simple_bit_table(outcome, bits):\n",
    "    n_bits = [len(b) for b in bits]\n",
    "    \n",
    "    return pd.DataFrame(\n",
    "        {'outcomes':outcome, \n",
    "         'bits':bits, \n",
    "         'n_bits':n_bits}).style.hide()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847f2f6b-7264-4205-9a7a-984fb2ca7fab",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print_simple_bit_table(['heads', 'tails'], ['0', '1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ce6667-a025-4589-91f1-64a9a0fa29c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = [0.5, 0.5]\n",
    "n_bits = [1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be5dacb-e851-4724-9a85-842a6be069dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**PROMPT:** Let's move up. If we want to represent the sample space of two coin tosses in bits, how do we do it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1016b35b-bd3b-4470-9720-746d72460417",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**PROMPT:** What is the sample space?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc823ae8-2741-4096-879a-40f28555652e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$A = \\{HH, HT, TT, TH \\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b51149-366a-47b4-a687-7066f4ce52e1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "two_toss_combinations = ['HH', 'HT', 'TT', 'TH']\n",
    "two_bit_combinations = ['00', '01', '10', '11']\n",
    "\n",
    "print_simple_bit_table(two_toss_combinations, two_bit_combinations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d129b5ec-2602-42b0-a954-63cdc6a54bf1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**PROMPT:** If someone did this experiment, how many questions would we need *on average* to find out what the outcome was?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f16fe8-9b91-43f6-a478-1f9a04e45448",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Two questions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1546c4-ed04-475a-a1a3-32405851439b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**First question:** Is the first throw heads? If yes, we have $\\{HH, HT\\}$ left. If no, we have $\\{TT, TH \\}$ left. \n",
    "\n",
    "**Second question:** Is the first throw heads? Whatever the answer, only one option will remain. \n",
    "\n",
    "Of course we could get lucky by asking just one question. Say, for example, the outcome was $HH$. If we ask \"was it heads and heads\", we would've needed only one question. But then we would've been lucky. *On average* we need two questions. This was the big innovation from Shannon. A ***bit is the number of yes-no questions we need to ask in order to know an outcome in an experiment***. This is why Gregory Bateson called a bit \"the difference that makes a difference\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73c5c4d-3e4e-42d1-8b23-bdc21a68c10a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Before we go on to formalize this discovery mathematically, let's build up some more intuition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60d675d-e371-4b23-bc92-d5a9e058d3b7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's consider an experiment with three trials or tosses. Typing that out is annoying, so we'll use a helper function for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d9f9cb-46c0-4644-b41f-017e00eae4bd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def produce_N_combinations(items, N):\n",
    "    return [''.join(x) for x in product(items, repeat = N)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9655d8-b3e9-4298-a818-921eccabe5a2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**PROMPT:** How big is the sample space of eight trials? You can use ```produce_N_compitations``` to find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5d8241-064a-4935-a5fe-c9f578090fe4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "three_toss_combinations = produce_N_combinations(['T', 'H'], 3)\n",
    "three_toss_combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87865eeb-ffae-451f-a633-93c3a2f50666",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**PROMPT:** How would we encode that in bits? Use ```produce_N_combinations``` again, if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67afab4a-321b-46de-a650-640ece196073",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "three_bit_combinations = produce_N_combinations(['0', '1'], 3)\n",
    "three_bit_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef6347c-bac1-4797-bc48-05cbe09e38c1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print_simple_bit_table(three_toss_combinations, \n",
    "                       three_bit_combinations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8289e25-c728-4ebd-bf0e-a416f060788e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**PROMPT:** How many questions do we need now?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddfd4fe-f830-4273-b66a-bd4952e4da65",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Three questions!\n",
    "\n",
    "**First question:** Is the first throw heads? If yes, we have $\\{HTT, HTH, HHT, HHH\\}$ left. If no, we have $\\{TTT, TTH, THT, THH \\}$ left. \n",
    "\n",
    "With four options left, we know from above we need just two more questions. So three questions in total!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5e75ee-a22b-482f-96a4-9a401ccd11bb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**PROMPT:** Do you notice some sort of connection between how we build our outcomes and the number of bits we need to describe the distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5390439c-3fdf-4385-9c1b-e0996aa03a4d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Yes, there is indeed a connection. And that's good, because all this counting can get tiresome... \n",
    "\n",
    "To know how many options we have to choose among, we can use a simple formula. If $S$ is the number of symbols we can choose from (i.e. 2, because we choose between $H$ and $T$) and $n$ is the number of combinations of those (3 in the above example), then the number of basic outcomes $E$ is:\n",
    "    \n",
    "$E = S^n$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03ef63f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d727260-8eed-4e36-afde-0bca5fc551d8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "S = 2\n",
    "n = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f252907-2214-4c33-b612-f955ed33ab39",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "E = np.power(S, n)\n",
    "E"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabd3d93-3814-4647-ad5e-e0a3fe5d60f0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**PROMPT:** How many bits will we need to represent these options?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d202ae35-da4a-48da-89c8-00861117d7e6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If we already know $E$, we can find out how many bits we need by taking the logarithm of $E$, because the logarithm is simply the inverse of a power:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d14ab89-23b9-4cd9-bacd-386cf6442b3f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "np.log2(E)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ae743d-393a-4770-81e3-b0ef7f1d0724",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In fact, we can count the number of bits we need directly from $n$ and $S$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966c6f42-7d79-4474-9120-17700f3c07cd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n*np.log2(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cf4c32-4eba-4085-8d03-f90c04a5052d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is also how Ralph Hartley, an engineer at Bell Labs, defined information in a paper in 1928. For him information was just:\n",
    "\n",
    "$$\n",
    "n \\cdot \\log S\n",
    "$$\n",
    "\n",
    "But this isn't the definition of that history settled on, and *only* applies to uniform distributions. Before we move on, let's take a little detour."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ed8160-cea3-4dfe-b797-d3b6002658be",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Aside: What Does the Logarithm Do?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b59bfa-42ef-4741-bea1-5f43de91a6c8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We use $log_2$ because it assumes $S=2$. \n",
    "- If we use some other logarithm, it's no longer a bit!\n",
    "- Using a logarithm of the natural number $e$ would produce \"nats\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff2e540-289c-437a-9a53-541b2a77fac7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The logarithm to the base 2 of a number is the power to which 2 must be raised to equal the number\n",
    "- We can express the same thing mathematically:\n",
    "\n",
    "$$\n",
    "2^{\\log_2 x} = x\\\\\n",
    "$$\n",
    "\n",
    "- With our three tosses of heads and tails:\n",
    "\n",
    "$$\n",
    "\\log_2 8 = 3 \\\\\n",
    "2^{\\log_2 8} = 8 \\\\\n",
    "2^3 = 8 \n",
    "$$\n",
    "\n",
    "Two must be raised to $3$ to give us 8."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6063b8eb-7b73-426e-85de-0962b5c31478",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**PROMPT:** What are the logarithms of the following numbers: 1, 2, 4, 8, 16, 32, 64?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e112bc8e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "| $\\mathbf{x}$  | $\\mathbf{\\log_2 x}$ | why?       |\n",
    "|---------------|---------------------|------------|\n",
    "| $1$           | $0$                 |$$2^0 = 1$$ |\n",
    "| $2$           | $1$                 |$2^1 = 2$   |\n",
    "| $4$           | $2$                 |$2^2 = 4$   |\n",
    "| $8$           | $3$                 |$2^3 = 8$   |\n",
    "| $16$          | $4$                 |$2^4 = 16$  |\n",
    "| $32$          | $5$                 |$2^5 = 32$  |\n",
    "| $64$          | $6$                 |$2^6 = 64$  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d896a06e-d4fc-4145-82e2-a4d9156e44ec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,9))\n",
    "plt.plot([1,2,4,8,16,32,64, 128, 256], np.log2([1,2,4,8,16,32,64, 128, 256]))\n",
    "plt.xlabel(\"x\", fontsize=16); plt.ylabel(r\"$\\log_2$ x\", fontsize=16); plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.title(r\"$\\log2$ of X $\\geq 1$\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43662f40-dd1e-4f6e-b626-4817861a0ff0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bits for Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ad13bf-760d-4a68-8dad-93974a944812",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So far we have worked with *counts*, but we started out asking how we could encode *probabilities*. How could we make that leap? An interesting property with the logarithm, is that taking the log of a fraction gives us the same result as taking the log of the denominator *but negative*. So\n",
    "\n",
    "$$\n",
    "\\log_2 2 = 1\n",
    "$$\n",
    "\n",
    "And:\n",
    "\n",
    "$$\n",
    "\\log_2\\frac{1}{2} = - 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c0718d-70cd-48b9-9501-0a07dec61b87",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This means that counting logs for a uniform distribution where all events are equally probable is just like working with counts, but negative!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe4dacb-b962-4435-b91d-e8d2e5ed912f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**PROMPT:** What are the logarithms of the following numbers: $\\frac{1}{2}, \\frac{1}{4}, \\frac{1}{8}, \\frac{1}{16}, \\frac{1}{32}, \\frac{1}{64}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793685c9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "| $\\mathbf{x}$  | $\\mathbf{\\log_2 x}$ | why?       |\n",
    "|---------------|---------------------|------------|\n",
    "| $1$           | $0$                 |$$2^0 = 1$$ |\n",
    "| $\\frac{1}{2}$           | $-1$                 |$2^{-1} = \\frac{1}{2}$   |\n",
    "| $\\frac{1}{4}$           | $-2$                 |$2^{-2} = \\frac{1}{4}$   |\n",
    "| $\\frac{1}{8}$           | $-3$                 |$2^{-3} = \\frac{1}{8}$   |\n",
    "| $\\frac{1}{16}$          | $-4$                 |$2^{-4} = \\frac{1}{16}$  |\n",
    "| $\\frac{1}{32}$          | $-5$                 |$2^{-5} = \\frac{1}{32}$  |\n",
    "| $\\frac{1}{64}$          | $-6$                 |$2^{-6} = \\frac{1}{64}$  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db992286-6a70-44a1-a51a-85e5a8926279",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "plt.plot([1/256, 1/128, 1/64, 1/32, 1/16, 1/8, 1/4,1/2, 1], \n",
    "         np.log2([1/256, 1/128, 1/64, 1/32, 1/16, 1/8, 1/4,1/2, 1]))\n",
    "plt.xlabel(\"x\", fontsize=16); plt.ylabel(r\"$\\log_2$ x\", fontsize=16); plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.title(r\"$\\log_2$ of $0 >$ x $\\leq 1$\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f60893b-8f09-4773-bb0f-a0fb4f4d280b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## From Bits to Information Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3036b002-f16e-4507-b971-87b1ec0f4bcf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We are now ready to understand the mathematical concept of **information entropy**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d9188a-4bb7-472a-8a13-6e33ab92f860",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To do so, let's return to the unbiased coin, once again. We already know we need two bits to encode it. BUt what if we wanted to calculate this value just from knowing the probabilities:\n",
    "\n",
    "$$\n",
    "P(H) = \\frac{1}{2} \\;\\;\\;\n",
    "P(T) = \\frac{1}{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3489f35-c4b2-4d52-8b1d-04ea32f9b96d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we take the log of these, we get:\n",
    "    \n",
    "$$\n",
    "\\log_2\\frac{1}{2} + \\log_2\\frac{1}{2} = \\\\\n",
    "-1 + (-1) = \\\\ \n",
    "-2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6cd364-6f2c-432e-a31d-3c20161d3c5d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But that's too much! And negative! We know the bits to encode a fair coin is one. What we need to do is to weigh each event in our distribution by it's probability to get not just a sum but a **\"weighted sum\"**. \n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\left(\\frac{1}{2}\\times\\log_2\\frac{1}{2}\\right) + \\left(\\frac{1}{2}\\times\\log_2\\frac{1}{2}\\right) = \\\\\n",
    "\\left(\\frac{1}{2}\\times(-1)\\right) + \\left(\\frac{1}{2}\\times(-1)\\right) = \\\\\n",
    "-\\frac{1}{2} -\\frac{1}{2} = \\\\\n",
    "-1\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a433299d-c2ca-4124-b701-7cf9886ed241",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Better! But still negative. What we do is just reverse the sign of the equation, like this:\n",
    "    \n",
    "$$\n",
    "\\begin{equation*}\n",
    "-1 \\times \\left[\\left(\\frac{1}{2}\\times\\log_2\\frac{1}{2}\\right) + \\left(\\frac{1}{2}\\times\\log_2\\frac{1}{2}\\right)\\right] = \\\\\n",
    "-1 \\times \\left[\\left(\\frac{1}{2}\\times(-1)\\right) + \\left(\\frac{1}{2}\\times(-1)\\right)\\right] = \\\\ \n",
    "-1 \\times \\left[-\\frac{1}{2} -\\frac{1}{2}\\right] = \\\\\n",
    "-1 \\times (-1) = \\\\\n",
    "1\n",
    "\\end{equation*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7651bff4-3d33-4c42-8798-dc104494f630",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can actually use this to take a stab at a first definition of **information entropy**. We will use the symbol $\\mathbb{H}$. So let's imagine a distribution $X$ with two outcomes $x_1$ and $x_2$ and let's for simplicity's sake say they have probabilities $p_1$ and $p_2$. Then, the information entropy is:\n",
    "\n",
    "$$\n",
    "\\mathbb{H}(X) = -(p_1 \\log_2 p_1 + p_2 \\log_2 p_2))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e217cbf8-de3c-46ef-98c1-bc6cbf9cb47f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And we can actually generalize this to a case with any number of outcomes and with any probability values, like this:\n",
    "    \n",
    "$$\n",
    "\\begin{equation}\n",
    "  \\mathbb{H}(X) = -\\overbrace{\\sum_{i=1}^N P(x_i)}^\\text{Weighted sum} \\underbrace{\\log_2 P(x_i)}_\\text{Bits}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7ffa23-1197-4498-84aa-9fae96f58e71",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's unpack again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425907ab",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1) First, the intimidating symbol $\\sum_{i=1}^N$ just states that we sum over all the probabilities in our distribution, with $N$ outcomes $x_i$, each of them indexed with $i$. It's like going over all the values in a list of length $N$ in a for-loop and adding them, just like the symbol $\\prod$ did for multiplication. Because we are summing probabilities, what we get is a weighted sum, just like we wanted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8807e350-ea6a-433d-81e1-8886c32a8bf0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2) Second, we calculate the bits required to encode each outcome. This we already went over above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18589ab2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "3) Third, we take the negative value of the whole thing, because the logarithms of fractions are negative (and probabilities are always fractions, except at 0 and 1) and we want the actual bits, not a negative value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8149e6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "4) The $(X)$ after $\\mathbb{H}$ just says that we are going over an entire distribution. It's notation for \"random variable\", which for our purposes is synonymous with distribution. It's not necessary, but you will see that notation quite often when doing machine learning, so it's good to know it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9545227-5335-4675-b0a4-759ebfc18c41",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**PROMPT:** How would you code entropy if you were to use a for loop? **Hint:** You will need to use ```np.log2```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3adf4df-bfa4-4f19-b8c9-d82d63fc5300",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Great! Let's wrap that in a function. $\\log_2 0$ is undefined, so we need to handle that separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e652d51-a6cd-4f8e-9f91-ecadede09391",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def entropy_as_loop(prob_dist):\n",
    "    entropy = 0\n",
    "    for p in prob_dist:\n",
    "        if not p == 0:\n",
    "            entropy += p * np.log2(p)\n",
    "        else:\n",
    "            entropy += 0\n",
    "    return -entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11697dbc-58dc-4195-bae7-1a53e1c0b09f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's now go over all these distributions and get the entropy for each of them. We'll need a list of distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4eb147",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "prob_distributions = list(zip(p_heads, p_tails))\n",
    "prob_distributions[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64b15d3",
   "metadata": {},
   "source": [
    "**PROMPT** Get the entropy for each distribution in ```prob_distributions```. Save the output as a ```list```. <div>\n",
    "**Remember:** Each tuple in this list is a probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b480711-6fc6-4f9c-993c-117f42e16475",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "coin_entropies = [entropy_as_loop(p_dist) for p_dist in prob_distributions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b15d59-5327-4357-840d-2065822116b2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_entropy(coin_entropies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ece0e89-9550-47b0-9ea7-292eb2584f1a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can also write our function in numpy, without the for-loop. It's a good exercise to do this, because for these types of functions people will rarely use for-loops."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f3f43c-20da-4268-8961-49527c25e3bd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**PROMPT:** Re-write the entropy function in ```numpy```. <br>\n",
    "**Hints:** \n",
    "- It's one line of code for the entropy itself, but...\n",
    "- ... if you want to handle the zeroes (you don't have to), you can do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bba91e8-d41b-4ab9-b433-c6a83795393b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "p = np.array([0,1])\n",
    "p = p[p > 0]\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c416a9-0a09-432f-a63c-734e69fa903b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def entropy_np(p):\n",
    "    p = np.array(list(p))\n",
    "    p = p[p > 0]\n",
    "    return -np.sum(p * np.log2(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a13171-6134-48cb-a78e-b227233ecfe1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's plot it to make sure it's the same as the for-loop version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741a35e3-082e-42ce-a59c-676aa25ca1e7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "coin_entropies = [entropy_np(prob_dist) for prob_dist in prob_distributions]\n",
    "\n",
    "plot_entropy(coin_entropies, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa534fe-586d-4683-ab9e-14375d4239c7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " And indeed it is! We've also added a red dot to highlight where the entropy as it's highest. \n",
    " - It's the point where $P(H)=0.5$, i.e. where $P(T)$ is also equal to $0.5$. \n",
    " - This is where the distribution is uniform. \n",
    " - This is why Weaver and Shannon called this least informative distribution the **\"maximum entropy\" distribution**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4ad3c7-15d0-4562-9fe8-4225a7a9f670",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Why is entropy maximized where the distribution is uniform? \n",
    "- Because this is where there is no structure, no redundancy.\n",
    "- All events are equally likely. \n",
    "- This hightlights Shannon's idea that information measures **suprise**. \n",
    "- When there is no structure, all events are equally surprising, so \"average surprise\" is maximized!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad175fed-9c52-48c5-98ee-1cbe4c57669f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Entropy is *not* at it's highest where probability is at it's highest, because:\n",
    "\n",
    "1) Probability measures the properties of *individual* events\n",
    "2) Information entropy measures the properties of *entire distributions*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac353ed7-e40e-4a43-92ca-e11153de7a3f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Bits for (a very simple) Language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d32ea42-0411-4daa-9298-f3bd3d5d2ffd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's now imagine an artificial language with only four letters, all equally probable. The letters are our unknown $x_i$ variable, and we index them with $i$ like this:\n",
    "\n",
    "$$\n",
    "x_1 = A, \\; x_2 = B, \\; x_3 = C, \\; x_4 = D\n",
    "$$\n",
    "\n",
    "They also have corresponding probabilities, so that \n",
    "\n",
    "$$\n",
    "p(x_1) = 0.25, \\; p(x_2) = 0.25, \\; p(x_3) = 0.25, \\; p(x_4) = 0.25\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8d71dc-5a37-4558-89cb-faaa60b976f9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "language = ['A', 'B', 'C', 'D']\n",
    "uniform_probs = [0.25]*4\n",
    "uniform_probs\n",
    "#lang_probs = [0.5, 0.25, 0.125, 0.125]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7066db47-e016-4489-bbbb-4236bfaa9c43",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's plot the language as a distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945b6e4f-6ce9-48c6-9ec0-77e965aff5b5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(uniform_probs)\n",
    "plt.xticks([0, 1, 2, 3], labels=language)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672c8957-78a6-4fa5-b1ed-5c19ee0011f2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Our language has a uniform distribution, so the line is flat. We've seen this type of event before: \n",
    "- When we were tossing our fair coin two times, we had four possible outcomes, just like now. \n",
    "- The average bits we needed to encode those outcomes were $2$. \n",
    "\n",
    "Let's see if that's true now as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a83039-de95-4b1d-9768-d4db1f1ee68b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "entropy_np(uniform_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af9bd55-dc14-4042-a3ed-5786c02887ed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "No surprises here. Or, from Shannon's perspective, **only surprises**! \n",
    "- Each outcome requires two bits to encode. \n",
    "- To clarify, we can re-write our code for the bit table to include the probability of the events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fc0af3-8e6b-469a-91ac-db76bd8d0cf5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def print_bit_table(outcome, bits, probs):\n",
    "    n_bits = [len(b) for b in bits]\n",
    "    \n",
    "    return pd.DataFrame(\n",
    "        {'outcomes':outcome, \n",
    "         'bits':bits, \n",
    "         'n_bits':n_bits,\n",
    "         'prob':np.round(probs, 3).astype(str)\n",
    "        }).style.hide()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d966eb-88b5-4450-889b-38bee89f4b0d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "uniform_bits = ['00', '01', '10', '11']\n",
    "print_bit_table(language, uniform_bits, uniform_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a12f34-a9eb-4395-a109-41a4133f187e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But what if our language instead had the following probabilities:\n",
    "    \n",
    "$$\n",
    "p(x_1) = 0.5 \\\\ p(x_2) = 0.25 \\\\ p(x_3) = 0.125 \\\\ p(x_4) = 0.125\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ce5b67-30c4-4f94-8296-f812e9d7a943",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "language = ['A', 'B', 'C', 'D']\n",
    "lang_probs = [0.5, 0.25, 0.125, 0.125]\n",
    "sum(lang_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b96f6d5-8556-4b45-94cf-a5d81acc84e1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's plot that again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fc426b-a607-4145-b30e-47b4513d23f3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(lang_probs)\n",
    "plt.xticks([0, 1, 2, 3], labels=language)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7f0a16-2cac-406f-b8df-1e3e1c94f622",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Looks familiar? Well, we've just created a language that seems to follow Zipf's Law. It's more simple than an actual language to make analysis easier, but the idea is the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20cc322-c7a7-4394-80af-e5c41ad1d5a3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_zipf_dist(alice_probs, 'line')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d64660-2633-47c7-8a46-364c2884eeda",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we now want to encode this language in bits, how do we go about it? Well, for starters we know how many bits we need on average. That knowledge is provided by our entropy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246d1957-35d7-4aff-b9f5-a8eee2b6ce1d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lang_entropy = entropy_np(lang_probs)\n",
    "lang_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b440f8d-f38a-4890-b5be-3f563383ba96",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But we know more than that, each outcome needs a number of bits that corresponds to it's probability:\n",
    "\n",
    "$$\n",
    "\\text{bits for } A = -\\log p(A) = -\\log\\frac{1}{2}=1 \\\\\n",
    "\\text{bits for } B = -\\log p(B) =-\\log\\frac{1}{4}=2 \\\\\n",
    "\\text{bits for } C = -\\log p(C) =-\\log\\frac{1}{8}=3 \\\\\n",
    "\\text{bits for } D = -\\log p(D) =-\\log\\frac{1}{8}=3\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efe096f-a070-4b01-89c1-e4fc796fd335",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Does this work? Let's try!\n",
    "\n",
    "**PROMPT:** Calculate how many bits we have on average, if we weigh each by their corresponding probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa785f0-fe79-476f-ba75-8fc6cbfe4ba1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "average_bits = (0.5 * 1 + 0.25 * 2 + 0.125*3 + 0.125*3)\n",
    "average_bits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58289f9a-fdb4-45c5-9894-6f85415c708d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It matches the entropy! Let's make that into a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ce8128-ca23-40b4-b61c-da56125d4604",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def average_bits(probs, n_bits):\n",
    "    return np.sum(np.multiply(probs, n_bits))\n",
    "\n",
    "n_lang_bits = [1,2,3,3]\n",
    "average_bits(lang_probs, n_lang_bits) == entropy_np(lang_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2e0d01-7ee0-4afe-b911-bd40b3c82d62",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we output our bit table with these values, we see what such an encoding could look like.\n",
    "\n",
    "PROMPT** Create the list ```lang_bits``` with four entries. Make these correspond to the bits each letter should have:\n",
    "\n",
    "$$\n",
    "A:1, \\; B:2, \\; C:3 \\; D:3 \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e141c3d-7c97-4f3c-a717-c560d8298add",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lang_bits = ['0', '10', '110', '111']\n",
    "print_bit_table(language, lang_bits, lang_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56984b50-f93c-47d5-8ffa-7a4e7290025c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Comparing Distributions: Relative Entropy & Redundancy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a66397f-569c-4190-85bf-472d0a24d65e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's reiterate what we've learned so far.\n",
    "\n",
    "First, we've learned to calculate:\n",
    "\n",
    "1. The bits to encode the outcomes of *one* distribution\n",
    "2. The average bits for this encoding\n",
    "\n",
    "Second, we know that:\n",
    "\n",
    "1. The information entropy of *one* distribution consists of the weighted sum of bits.\n",
    "2. Entropy measures surprise. The more uniform, the higher the entropy. An entirely uniform distribution is the \"maximum entropy\" distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e914936-9766-40ee-bd1a-4e90753dab02",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But what if we want to compare distributions? What if we want to somehow measure the entropy of their difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8414f0-e293-4e7a-872d-e70ac8ac41e2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's start with two familiar examples. \n",
    "- We have our language with $A$, $B$, $C$ and $D$. \n",
    "- We know their probabilities. \n",
    "- We also know what it would look like if we assumed an uniform distribution for them. \n",
    "\n",
    "Let's now compare these two distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd32076-5597-44bf-a1fa-b03c6e713cb9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "compare_distributions_in_plot(language, \n",
    "                              lang_probs, \n",
    "                              uniform_probs,\n",
    "                              'Actual',\n",
    "                              'Uniform',\n",
    "                              'Outcomes',\n",
    "                              'Probability',\n",
    "                              'Comparing the actual distribution of our language to a uniform distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9eb4e1f-aac6-4dba-a6d1-71c38e0f3b22",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "One way we could compare them, is to just take their ratio. We will call this ratio **\"relative entropy\"**:\n",
    "\n",
    "$$\n",
    "\\text{Relative entropy} = \\frac{\\text{Entropy of a distribution}}{\\text{The max entropy of the same distribution}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755c8fde-c556-4266-b15c-330a33d49504",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**PROMPT:** Can you write that in Python for our example above? Wrap it in a function like this: ```def relative_entropy(true_ent, max_ent)```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc90f77-24d2-41d6-b2b0-189da3b34729",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Recall the entropies of these two distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e410cd99-8504-4d1d-93c4-9701914648be",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "uniform_entropy = entropy_np(uniform_probs)\n",
    "uniform_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72951179-b47c-45d2-a504-fa27e7bc7c70",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lang_entropy = entropy_np(lang_probs)\n",
    "lang_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726fcc95-b6bd-48ef-8d33-394f67e4a179",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def relative_entropy(true_ent, max_ent):\n",
    "    return true_ent / max_ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caacfd6d-0069-498e-a814-656e431194b3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "relative_entropy(lang_entropy, uniform_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d872db30-9ca1-40a6-9bd3-39049a8b67cb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Shannon introduces this concept in his work to describe the difference between the entropy of our source and a completely uniform version of the same source. In Shannon's (p. 56) own words:\n",
    "\n",
    "> The ratio of the entropy of a source to the maximum value it could have while still restricted to the same symbols will be called its *relative entropy*. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5347f7a4-137e-4ea1-b584-4aa770e1b63f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What does relative entropy capture then? Weaver gets into this question in his intro to Shannon's work:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ef1f1b-708b-4005-8008-d4539fa64356",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> If the relative entropy of a certain source is, say .8, this roughly means that this source is, in its choice of symbols to form a message, about 80 per cent as free as it could possibly be with these same symbols. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d1ae75-08be-46ea-b6ab-c995dc198a5f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Shannon himself did a bunch of experiments to conclude that the redundancy of written English is about 50%. In his opinion:\n",
    "\n",
    "> This means that when we write English half of what we write is determined by the structure of the language and half is chosen freely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76265929-c6fa-410b-baaf-118b0f1a8a24",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One way to better understand what relative entropy captures, is by looking at it's relationship to **redundancy**. Shannon defined **redundancy** in terms of relative entropy as:\n",
    "\n",
    "$$\n",
    "\\text{Redundancy }= 1-\\text{ Relative Entropy}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed895c06-eea0-4c31-87a6-feb74e1953ec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Relative entropy tells us how much freedom we have and redundancy tells us how much we lack. The uniform distribution is total freedom: Everything is equally possible! Once we add structure, our choices become more constrained. According to Shannon, redundancy\n",
    "\n",
    "> is the fraction of the structure of the message which is determined not by the free choice of the sender, but rather by the accepted statistical rules governing the use of the symbols in question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a19ad8-5041-483a-a4d4-f396d39e2fb3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are some interesting philosophical implications here: Our use of language is just the actualization of the virtual space of possible outcomes, governed by statistical rules. A decisively posthuman vision of communication!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04725ad-74da-484e-b4ff-e7cedc1d7969",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**PROMPT:** What is the redundancy of our artificial language?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4642dd-c256-4152-8619-7d25c740d57a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If we go back to our actual data, we can see that our language has 0.875 relative entropy and 0.125 redundancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0166c8ee-d1be-4b57-ba82-c2fa9452bb57",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "relative_entropy(lang_entropy, uniform_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d119164-eabd-444f-9bac-fd15076febe7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "1-relative_entropy(lang_entropy, uniform_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49daa5f-51d9-4f0c-9d64-3c6a04c380cc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "While Shannon and Weaver limited their analysis of relative entropy to the ratio between the entropy of a source and its max entropy, the principle of calculating such ratios can be extended.\n",
    "\n",
    "Let's now instead say that we are familiar with the language $A$, $B$, $C$ $D$, but we don't quite know what the probabilities for each outcome are. Based on our prior knowledge, we would guess it is something like this:\n",
    "\n",
    "$$\n",
    "p(A) = 0.625 \\\\ p(B) = 0.125 \\\\ p(C) = 0.125 \\\\ p(D) = 0.125\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25cc7f8-45ce-4f2f-ac7c-8d7d292cff96",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "assumed_probs = [0.625, 0.125, 0.125, 0.125]\n",
    "sum(assumed_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcea7d9-b04b-454f-8572-8be72af16588",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Comparing these visually, we see that it's an okay estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3227285-a032-4dfc-9e6c-501af383c4eb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "compare_distributions_in_plot(language, \n",
    "                              lang_probs, \n",
    "                              assumed_probs,\n",
    "                              'Actual',\n",
    "                              'Assumed',\n",
    "                              'Outcomes',\n",
    "                              'Probability',\n",
    "                              'Comparing the actual distribution of our language to our assumed distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a73f43a-e054-41b7-b3e2-d0966200291e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The entropy for our estimate is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2753cde0-e9e6-4ae8-98fa-f0222cfbc45c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "assumed_entropy = entropy_np(assumed_probs)\n",
    "assumed_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beadbde1-e1fa-4f56-b314-ca7bbcfa4a75",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And compared to you actual entropy, we see that our assumed language is actually more structured. The high probability given to $A$ makes it more deterministic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1fa6d1-4fd1-450e-8f2b-d8f8ef6d1421",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "relative_entropy(lang_entropy, assumed_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539c9d46-f029-498f-b954-a651f61e32cf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's stop for a moment to recall what we set out to do: We wanted to find a metric to measure the difference between different probability distributions. And now we've done?!\n",
    "\n",
    "Well yes, kind of. But there's a problem. \n",
    "\n",
    "**PROMPT:** What might isse be with using relative entropy to compare distributions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c0514d-e37f-4ca9-89e5-d767b2638a74",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Well, the problem is that relative entropy doesn't account for the individual probabilities of our outcomes. What kind of issues might this create? Let's clarify with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e514dd-0f04-4a58-807f-734bce2379bc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Imagine our language from above, but with inverse probabilities. So:\n",
    "\n",
    "$$\n",
    "p(A) = 0.125 \\\\ p(B) = 0.125 \\\\ p(C) = 0.25 \\\\ p(D) = 0.5\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1261bbc-ec90-40e7-b446-9dfd85c4bbce",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "inverse_probs = lang_probs.copy()\n",
    "inverse_probs.reverse()\n",
    "inverse_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708160e4-63bd-4060-a0d6-8775617ca87b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now we see that while our distributions have shaped that mirror each other, our estimate would surely produce terrible predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40126241-706f-4f95-b429-ad108aca0de2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "compare_distributions_in_plot(language, \n",
    "                              lang_probs, \n",
    "                              inverse_probs,\n",
    "                              'Actual',\n",
    "                              'Inverse',\n",
    "                              'Outcomes',\n",
    "                              'Probability',\n",
    "                              'Comparing the actual distribution of our language to a inverse distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6f2198-ca6b-42e5-a3ea-1ea68f70cf4c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**PROMPT:** What is the relative entropy between these two distributions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e636ebf2-3819-4b40-aec1-3f117e3f84d4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "reversed_entropy = entropy_np(inverse_probs)\n",
    "reversed_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7070df9-54b8-445d-98d9-ed561c097f65",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "relative_entropy(lang_entropy, reversed_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3ebe3b-e810-455c-8bb7-17af76531577",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "relative_entropy(lang_entropy, lang_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2e300f-4ea9-4ccc-bd11-324cbf62ca37",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Relative entropy says nothing about the actual shape of the distribution, it just tells what it's mean is. We will need sharper tools to actually compare distributions. But we will still use entropy!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460fddb7-57ad-43e9-b04f-3dff67282188",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Comparing Distributions: The Kullback-Leibler Divergence\n",
    "\n",
    "Today, no one uses relative entropy or redundancy in machine learning, at least not in the sense that they were defined by Shannon. Instead, people use metrics that were further developed from the idea of relative entropy by other people who built on Shannon's work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebac5d34-ac4c-4eb9-8904-2b12ba0536f6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One particularly important measure like this is the Kullback-Leibler Divergence or $\\mathbb{KL}$. \n",
    "- It was developed by mathemtaicians Solomon Kullback and Richard Leibler in a 1951 paper. \n",
    "- While they didn't name it after themselves, other people since then have started using this name. \n",
    "- It's from the $\\mathbb{KL}$ divergence that cross-entrop, the perhaps most commonly used tool for comparing distributions in ML, is derived."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e2e6da-b2e6-40cd-8c5c-2f3974a41bbe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's start with intuition again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5352235a-7476-45fb-93fa-8d47310fb80a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What if for every outcome $x_i$ in our distribution, we compared the number of bits we need to encode that outcome?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d92f15-8af9-4aa4-8c8f-b6e9f3cb4e4f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**PROMPT:** Can you code such a for-loop in Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd67dd7-91af-4a48-9ee8-1e79c3cceb0a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "entropy_diff = 0\n",
    "\n",
    "for p in lang_probs:\n",
    "    entropy_diff += (np.log2(p) - np.log2(0.25))\n",
    "entropy_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08002522-5db2-44ca-86a7-dbb8328abef6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What? We actually need in total one bit more to encode our structured language than we need for the uniform distribution. And indeed, this is true, we see it from our bit tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09135edf-6f62-4d31-95b4-033286047a2a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print_bit_table(language, lang_bits, lang_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a5b30e-a29a-4326-a967-be88e1524cfa",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print_bit_table(language, uniform_bits, uniform_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d092a2f-4100-4d08-a5c3-a0274b10d4cb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Our structured language requires 9 bits to encode, our uniform distribution only takes 8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea793a65-91c7-4096-b16a-6b0117ed7174",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(sum(n_lang_bits))\n",
    "print(4*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00055379-94d2-4297-9ba2-c3753f2e6c92",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But entropy isn't about sums, it's about **weighted sums**, also known as **\"expected value\"** or, simply, **\"mean\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8474d8-d63e-4e1b-a8ef-87f93d2c475c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So if we rewrite our loop, but now weigh every difference. What should we weigh it by? \n",
    "- Well, how about the probabilities of the actual distribution we are interested in? \n",
    "- Weighing the loop with the probabilities of our language, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ee0db0-70a4-4d03-9a1a-68347bbfa855",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "entropy_diff = 0\n",
    "\n",
    "for p in lang_probs:\n",
    "    entropy_diff += p*(np.log2(p) - np.log2(0.25))\n",
    "entropy_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d453ef4e-115b-4ac4-a26d-bc2e2061dabf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What we now get is the distance of the uniform distribution from the vantage point of our the probabilities of our artificial language. And this is the $\\mathbb{KL}$ divergence! That's all there is to it. Let's write it in numpy and try it out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7910aeb5-8703-4f1c-8bd2-760640497ef3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**PROMPT:** Write the $\\mathbb{KL}$ divergence in ```numpy```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b321053-309e-468a-ae25-ec2bd287db27",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Hint:** Here you have it as a loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cc6df1-5ffe-4df1-b87b-1ef1955e6e0d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def kl_divergence_loop(p, q):\n",
    "    kld = 0\n",
    "    for idx, p in enumerate(lang_probs):\n",
    "        kld += p*(np.log2(p) - np.log2(q[idx]))\n",
    "    return kld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61a3a45-1d70-429c-8b07-e2e2ef55dbd9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def kl_divergence(p, q):\n",
    "    return np.sum(p * np.log2(p / q), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49db7dfd-ee19-4e08-bde8-9bd296276faa",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Nice! Let's try it out:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08239975-6029-413d-93fc-8da7339a26ac",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Our uniform distribution is on average 0.25 bits away from our actual distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4822f1ff-993a-4f7c-9b5c-95e5cff164f0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "kl_divergence(np.array(lang_probs), np.array(uniform_probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95fed96-5d49-48d0-b061-245183202870",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Why is this true? Because, as we saw above, their total distance is $1$, so with four outcomes the average distance is $\\frac{1}{4} = 0.25$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65be93d2-7669-4774-9b68-d16d49e081c5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Our estimate for the language is already a lot closer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619c0fb8-6663-44eb-9f49-e297acbcbeac",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "kl_divergence(np.array(lang_probs), np.array(assumed_probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9468bd95-181d-4521-bf7d-afc3874528ac",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Then, if we compare our language to itself, the distance is $0$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d590b7f9-9d18-467d-9309-40a3746ee4a0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "kl_divergence(np.array(lang_probs), np.array(lang_probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59b2aeb-2b90-431f-9060-90937736be0c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This means that we know exactly what distribution $P$ generated the data and the difference in bits needed to encode the distributions is zero. In other words, \n",
    "\n",
    "> it means that we can correctly predict the probabilities of all possible future events, and thus we have learned to predict the future as well as an ’oracle’ that has access to the true distribution P (Murphy 2021, 243)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed54431-946c-4684-8430-729d6371c1e7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Finally, how does the $\\mathbb{KL}$ divergence do on our inverse distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057a6abb-0730-4fa6-828b-2eb451bc3ec2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "kl_divergence(np.array(lang_probs), np.array(inverse_probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b616abdd-8cd1-4d7f-a417-b8812ddde9a5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Quite well, it turns out! Whereas relative entropy was unable to distinguish them, $\\mathbb{KL}$ divergence shows that they are further away from each other than any other distributions we compared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcc9d5b-8fb3-4e26-aa05-bd6c8bbdd062",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If you felt like getting here was surprisingly easy, take a look at how $\\mathbb{KL}$ divergence is derived in a typical textbook:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb592154-6269-430b-8466-3da4d0ee1028",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "{\\begin{equation}\n",
    "    \\begin{aligned}\n",
    "        \\mathbb{KL}(p \\vert\\vert q) &= \\overbrace{E\\bigg[\\log_2 p(x_i) - \\log_2 q(x_i)\\bigg]}^\\color{red}{\\text{Expected surpise of p when encoding with q}} \\\\\n",
    "         &= \\underset{i=1}{\\sum}p(x_i) \\cdot \\bigg[\\log_2 p(x_i) - \\log_2 q(x_i)\\bigg] \\\\\n",
    "         &= \\underset{i=1}{\\sum} p(x_i) \\log_2 \\frac{p(x_i)}{q(x_i)} \\\\\n",
    "         &= \\underset{i=1}{\\sum} p(x_i)\\log_2 p(x_i) - \\underset{i=1}{\\sum} p(x_i)\\log_2 q(x_i) \\\\\n",
    "         &= \\underbrace{-\\mathbb{H}(p)}_\\text{Negentropy of p} + \\underbrace{\\mathbb{H}(p,q)}_\\text{Cross-entropy between p and q}\n",
    "     \\end{aligned}\n",
    "\\end{equation}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9eedc6f-6884-400c-868d-7982c3f2564e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Looks hard? Well, sure. But mostly because we aren't familiar with the notation and the associated rules of different symbols. Building up the intuition slowly and in code, it's hopefully more clear :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b38053d-ea5f-4ecf-920c-b43ee810af03",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we can move on to our last part: **cross-entropy**. This function is the last terms of the $\\mathbb{KL}$ divergence above and probably the most commonly used optimization function in neural networks today."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00de46b-4dd5-4038-9850-fe8e08346ba5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Comparing Distributions: Cross-Entropy\n",
    "\n",
    "With cross-entropy, we take a step back and go to the original definition of entropy. \n",
    "- What if we encoded our artificial language with bits corresponding to some other language. \n",
    "- For example: What if we used the bits for the uniform distribution to encode the artificial language? \n",
    "    - How many bits would we then need on average. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faa437f-031c-4d98-873e-ff2f94cf3504",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's say the distributions $P$ and $Q$ have the same outcomes $x_i$, but with different probabilities $p(x_i)$ and $q(x_i)$. Then, in terms of our equation for information, it would look like this:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "  \\mathbb{H}(P,Q) = -\\overbrace{\\sum_{i=1}^N p(x_i)}^\\text{Weighted sum for P} \\underbrace{\\log_2 q(x_i)}_\\text{Bits for Q}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420ab262-285d-4ba8-b047-9ef485cf16c7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can modify our $\\mathbb{KL}$ divergence loop accordingly.\n",
    "\n",
    "**PROMPT:** Write cross-entropy with a for-loop or in ```numpy```. <div> **Hint:** You can use both the for-loop and ```numpy```implementations of $\\mathbb{KL}$ divergence and the equation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66261195-574c-439f-8fc4-8826c07f9897",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def cross_entropy_loop(p, q):\n",
    "    ce = 0\n",
    "    for idx, p in enumerate(lang_probs):\n",
    "        ce += p*(- np.log2(q[idx]))\n",
    "    return ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cdb8ba-8c7a-407b-8564-67ba58db07f6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cross_entropy_loop(lang_probs, uniform_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "073d98fd-4f47-4577-8975-3941f4a45d8a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def cross_entropy_np(p, q):\n",
    "    ce =  -np.sum(p * np.log2(q))\n",
    "    return ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3aa6595-a7b6-4a6a-81d9-60a41202f8d2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cross_entropy_np(lang_probs, uniform_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f534cd-15bd-4cfc-9b30-2d2b3d0cb4ef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What do these numbers tell us? Well, just how many bits we need on average to encode our artificial language if we instead assume it is uniform. For our estimate, it's already lower:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138abc36-e559-4c1a-9ad7-7c06baf024cc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cross_entropy_np(lang_probs, assumed_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374b144d-28d1-41ba-a27e-cd54e669f052",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But the most efficient encoding is achieved using the actual distribution of the langauge itself. And, indeed, the cross-entropy between our language and itself is just the entropy of the language! Like plain old entropy, it tells us how many bits we need on average to code the language with it's own encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c57f510-00d0-4593-b8f2-f26e200e80a0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cross_entropy_np(np.array(lang_probs), np.array(lang_probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393364e4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And that's it! \n",
    "- You now know as much if not a lot more information theory as most people doing machine learning. \n",
    "- Most people don't *really* understand cross-entropy, they just use it :-S"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
